{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wbcbtRrfpggr"
   },
   "source": [
    "# Στειροπούλου Ευαγγελία \n",
    "### Α.Μ. 111520180016\n",
    "## Εργασία 2η Αναγνώριση Προτύπων - Μηχανική Μάθηση\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AZbVKoO5EXn0"
   },
   "source": [
    "# Face recognition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "byw3u5KBqwos"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.24.3\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.7.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./myenv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting importlib-resources>=3.2.0; python_version < \"3.10\"\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in ./myenv/lib/python3.8/site-packages (from matplotlib) (1.24.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./myenv/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.15.0)\n",
      "Installing collected packages: cycler, pyparsing, importlib-resources, contourpy, pillow, kiwisolver, fonttools, matplotlib\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 importlib-resources-5.12.0 kiwisolver-1.4.4 matplotlib-3.7.1 pillow-9.5.0 pyparsing-3.0.9\n",
      "Processing /home/eva/.cache/pip/wheels/44/08/18/d0b86f591e929e063b3134b126c8a77b3758e527fe1a3f6fb8/sklearn-0.0.post5-py3-none-any.whl\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0.post5\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdecomposition\u001b[39;00m \u001b[39mimport\u001b[39;00m PCA\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install matplotlib\n",
    "!pip3 install -U scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import glob\n",
    "import re\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, ConfusionMatrixDisplay, classification_report\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "emJ60JYeIle7"
   },
   "source": [
    "## I: loadImages function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path, set_number):\n",
    "    image_paths = glob.glob(path + \"/person*_*.png\")\n",
    "\n",
    "    image_matrix = []\n",
    "    labels = []\n",
    "    for path in image_paths:\n",
    "        filename = path.split('/')[-1]\n",
    "        person_id = int(filename.split('_')[0][6:])\n",
    "        image_number = int(filename.split('_')[1].split('.')[0])\n",
    "\n",
    "        if ((set_number == \"Set_1\" and 1 <= image_number <= 7) or\n",
    "            (set_number == \"Set_2\" and 8 <= image_number <= 19) or\n",
    "            (set_number == \"Set_3\" and 20 <= image_number <= 31) or\n",
    "            (set_number == \"Set_4\" and 32 <= image_number <= 45) or\n",
    "            (set_number == \"Set_5\" and 46 <= image_number <= 64)):\n",
    "\n",
    "            image_array = np.array(plt.imread(path).flatten())\n",
    "            image_matrix.append(image_array)\n",
    "            labels.append(person_id)\n",
    "    \n",
    "    return np.array(image_matrix), labels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Όπως αναφέρεται στην εκφώνηση, η προεπεξεργασία της εικόνας είναι απαραραίτητη, και γίνεται με την αφαίρεση της μέσης τιμής της και στην συνέχεια με την διαίρεση με την τυπική απόκλιση των τιμών της."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(faces):\n",
    "    X = np.ndarray(shape=faces.shape)  # Normalized data matrix\n",
    "    for i in range(faces.shape[0]):\n",
    "        X[i] = (faces[i] - np.mean(faces, axis=0)) / np.std(faces, axis=0)\n",
    "    return X"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II: Train Eigenfaces method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set1, y_set1 = loadImages(\"/home/eva/Documents/machine_learning/project2/faces\", \"Set_1\")\n",
    "   \n",
    "X_train_set_1 = normalization(X_set1) #Train the model with the first set of images\n",
    "\n",
    "#for dimensions 9 and 30\n",
    "dimensions = [9, 30]\n",
    "#create a dictionary to store the results of the accuracy for every set of images\n",
    "accuracy = {}\n",
    "for d in dimensions:\n",
    "    print(\"Dimension: \" + str(d))\n",
    "    pca = PCA(n_components=d, whiten=True) #Apply PCA\n",
    "    X_train_pca = pca.fit_transform(X_train_set_1) #Fit the model using the initial training set(set 1)\n",
    "    knn = KNeighborsClassifier(n_neighbors=1).fit(X_train_pca, y_set1)  #Train the model\n",
    "    for set_number in range(1,6):\n",
    "        #Test the model, for every set of images\n",
    "        X_set, y_set = loadImages(\"/home/eva/Documents/machine_learning/project2/faces\", \"Set_\" + str(set_number))\n",
    "        X_test_set = normalization(X_set)\n",
    "        X_test_set = pca.transform(X_test_set)\n",
    "        y_pred = knn.predict(X_test_set)\n",
    "        accuracy[str(set_number)] = accuracy_score(y_set, y_pred)*100\n",
    "        #print(\"Accuracy for dimension \" + str(d) + \" is: \" + str(accuracy))\n",
    "\n",
    "        #Plot the confusion matrix for the set of images\n",
    "        cm = confusion_matrix(y_set, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_set))\n",
    "        disp.plot()\n",
    "        print(\"Confusion matrix\")\n",
    "        plt.title(\"Accuracy for set \" + str(set_number) + \" is \" + str(accuracy[str(set_number)]) + \"%\")\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρατηρώ ότι για d = 30, η ακρίβεια ταξινόμησης είναι υψηλότερη για τα υπόλοιπα sets****************************. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III: 9 top eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the top 9 eigenfaces\n",
    "plt.figure(figsize=(9, 8))\n",
    "for i in range(9):  \n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    eigenfaces = pca.components_[i].reshape(50,50)\n",
    "    plt.title(\"Eigenface \" + str(i+1))\n",
    "    plt.imshow(eigenfaces, cmap='bone')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV: Recreate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every dimension recreate a random image for each of the 5 sets of images\n",
    "#and plot the initial image and the reconstructed image\n",
    "#dictionary to store the initial image, and the reconstructed image for both dimensions\n",
    "\n",
    "dimensions = [9, 30]\n",
    "initial_reconstructed = {}\n",
    "\n",
    "for set_number in range(1, 6):\n",
    "    X_set, y_set = loadImages(\"/home/eva/Documents/machine_learning/project2/faces\", \"Set_\" + str(set_number))\n",
    "    random_image = np.random.randint(0, X_set.shape[0])  # Choose a random image\n",
    "     \n",
    "    for i, d in enumerate(dimensions):\n",
    "        pca = PCA(n_components=d, whiten=True)  # Apply PCA with the current dimension\n",
    "        X_train_pca = pca.fit_transform(X_set)  # Fit PCA with the training data\n",
    "        original_image = normalization(X_set[random_image])  # Normalize the original image\n",
    "    \n",
    "        transformed_image = pca.transform(original_image.reshape(1, -1))  # Transform the image using the fitted PCA model\n",
    "        reconstructed_image = pca.inverse_transform(transformed_image).reshape(50, 50)  # Reconstruct the image using the fitted PCA model\n",
    "        reconstructed_image += np.mean(X_set1)  # Denormalize the reconstructed image\n",
    "        initial_reconstructed[str(set_number) + \"_\" + str(d) ] = [original_image, reconstructed_image]\n",
    "\n",
    "        plt.subplot(len(dimensions), 2, i*2 + 1)\n",
    "        plt.title(\"Initial image (Dimension \" + str(d) + \")\")\n",
    "        plt.imshow(original_image.reshape(50, 50), cmap='gray')\n",
    "\n",
    "        plt.subplot(len(dimensions), 2, i*2 + 2)\n",
    "        plt.title(\"Reconstructed image (Dimension \" + str(d) + \")\")\n",
    "        plt.imshow(reconstructed_image, cmap='gray')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H ανακατασκευή της εικόνας από το πρώτο σετ, το οποίο χρησιμοποιήθηκε για την εκπαίδευση είναι η πιο ακριβής, Στο 2ο σετ η ποιότητα είναι λίγο χαμηλότερη, αλλά αρκετά υψηλή. Με την αυξηση των διαστάσεων σε 30 απο 9 οι ανακατασκευασμένες εικόνες περιέχουν περισσότερες λεπτομέρειες.\n",
    "\n",
    "Στο τρίτο σετ εικόνων, η ποιότητα της ανακατασκευής αρχίζει να χειροτερεύει, ιδιαίτερα για d=9 όπου είναι αρκετά χαμηλή. Ωστόσο, για d=30 παρατηρούμε βελτίωση και εμφανίζονται περισσότερα χαρακτηριστικά του προσώπου, τα οποία είναι παρόμοια με αυτά του αρχικού προσώπου στην αρχική εικόνα.\n",
    "\n",
    "\n",
    "???????????????????????????????????\n",
    "Τέλος, στο τέταρτο και πέμπτο σετ εικόνων, η ανακατασκευή για d=9 δεν είναι καλή και πλησιάζει το \"average\" mean πρόσωπο του training set, καθώς δεν υπάρχει αρκετή πληροφορία. Ωστόσο, για d=30 παρατηρείται κάποια βελτίωση και υπάρχει περισσότερη λεπτομέρεια, αλλά παραμένει μακριά από την αρχική εικόνα. Αυτό είναι αναμενόμενο λόγω των διαφορετικών συνθηκών φωτισμού που υπάρχουν σε αυτά τα δύο τελευταία υποσύνολα εικόνων, τα οποία είναι τελείως διαφορετικά από το σετ 1 που χρησιμοποιήθηκε για την εκπαίδευση της μεθόδου."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V:9 singular vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the 9 singular vectors, for the first set of images after applying SVD\n",
    "U, S, V = np.linalg.svd(X_set1, full_matrices=False)\n",
    "\n",
    "#plot the top 9 eigenfaces\n",
    "print(\"Eigenvectors\")\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(1,10):\n",
    "  plt.subplot(1, 9, i)\n",
    "  plt.title('EV %d' %i, fontweight =\"light\")\n",
    "  plt.imshow(pca.components_[i-1].reshape(50, 50), cmap='bone')\n",
    "plt.show()\n",
    "\n",
    "#plot the top 9 singular vectors\n",
    "print(\"\\nSingular vectors\")\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(1,10):\n",
    "  plt.subplot(1, 9, i)\n",
    "  plt.title('SV %d' %(i-1), fontweight =\"light\")\n",
    "  plt.imshow(V[i-1,:].reshape(50, 50), cmap='bone')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Στα δεδομένα του PCA έχει γίνει normalization, όπου έχει αφαιρεθεί η μέση τιμή και έχει διαιρεθεί με την τυπική απόκλιση. Άρα από τα eigenvectors λείπει αυτη η πληροφορία, και οι διαφορές ανάμεσα στις εικόνες που προκύπτουν με τις δύο μεθόδους, οφείλονται σε αυτό."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_openml('mnist_784', version = '1', parser='auto')\n",
    "#transform every image to a 28x28 = 784 matrix\n",
    "print(dataset.data.shape)\n",
    "# Separate the features (X) and labels (y)\n",
    "X, y = dataset['data'], dataset['target']\n",
    "\n",
    "# Normalize the features to the range [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM vs RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=60000, test_size=10000, random_state=42)\n",
    "\n",
    "# Define the parameter combinations to evaluate\n",
    "kernel_types = ['linear', 'rbf']\n",
    "C_values = [0.1, 1, 10]\n",
    "gamma_values = [0.1, 1, 10]\n",
    "\n",
    "# Initialize variables to store the best parameters and accuracy\n",
    "best_accuracy = 0\n",
    "best_kernel = ''\n",
    "best_C = 0\n",
    "best_gamma = 0\n",
    "\n",
    "# Iterate over parameter combinations\n",
    "for kernel in kernel_types:\n",
    "    for C in C_values:\n",
    "        for gamma in gamma_values:\n",
    "            # Create an SVM classifier with the current parameters\n",
    "            svm = SVC(kernel=kernel, C=C, gamma=gamma)\n",
    "            \n",
    "            # Train the classifier\n",
    "            svm.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict labels for the test set\n",
    "            y_pred = svm.predict(X_test)\n",
    "            \n",
    "            # Calculate the accuracy\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # Check if the current combination has better accuracy\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_kernel = kernel\n",
    "                best_C = C\n",
    "                best_gamma = gamma\n",
    "\n",
    "# Print the best parameter values and accuracy\n",
    "print(\"Best kernel:\", best_kernel)\n",
    "print(\"Best C value:\", best_C)\n",
    "print(\"Best gamma value:\", best_gamma)\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
